{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch: Neural Network from Scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sachinkun21/PyTorch/blob/master/PyTorch_Neural_Network_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UotwZ748PdtW",
        "colab_type": "text"
      },
      "source": [
        "## **What is PyTorch?**\n",
        "Let’s understand what PyTorch is and why it has become so popular lately, before diving into it’s implementation.\n",
        "\n",
        "- PyTorch is a Python based scientific computing package that is similar to NumPy, but with the added power of GPUs. \n",
        "- It is also a deep learning framework that provides maximum flexibility and speed during implementing and building deep neural network architectures.\n",
        "\n",
        "\n",
        "Recently, PyTorch 1.0 was released and it was aimed to assist researchers by addressing four major challenges:\n",
        "\n",
        "1. Extensive reworking\n",
        "2. Time-consuming training\n",
        "3. Python programming language inflexibility\n",
        "4. Slow scale-up\n",
        "\n",
        "\n",
        "Intrinsically, there are two main characteristics of PyTorch that distinguish it from other deep learning frameworks:\n",
        "\n",
        "1. Imperative Programming\n",
        "2. Dynamic Computation Graphing\n",
        "\n",
        "**Imperative Programming:** PyTorch performs computations as it goes through each line of the written code. This is quite similar to how a Python program is executed. This concept is called imperative programming. The biggest advantage of this feature is that your code and programming logic can be debugged on the fly.\n",
        "\n",
        "**Dynamic Computation Graphing:** PyTorch is referred to as a “defined by run” framework, which means that the computational graph structure (of a neural network architecture) is generated during run time. The main advantage of this property is that it provides a flexible and programmatic runtime interface that facilitates the construction and modification of systems by connecting operations. In PyTorch, a new computational graph is defined at each forward pass. This is in stark contrast to TensorFlow which uses a static graph representation.\n",
        "\n",
        "PyTorch 1.0 comes with an important feature called torch.jit, a high-level compiler that allows the user to separate the models and code. It also supports efficient model optimization on custom hardware, such as GPUs or TPUs.\n",
        "\n",
        " \n",
        "\n",
        "Building Neural Nets using PyTorch\n",
        "Let’s understand PyTorch through a more practical lens. Learning theory is good, but it isn’t much use if you don’t put it into practice!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ4S-k2YP50M",
        "colab_type": "text"
      },
      "source": [
        "A PyTorch implementation of a neural network looks exactly like a NumPy implementation. The goal of this section is to showcase the equivalent nature of PyTorch and NumPy. For this purpose, let’s create a simple three-layered network having 5 nodes in the input layer, 3 in the hidden layer, and 1 in the output layer. We will use only one training example with one row which has five features and one target.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYLbSjiFDOY1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "n_input, n_hidden, n_output = 5 , 3 , 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT8-7j90QBWS",
        "colab_type": "text"
      },
      "source": [
        "The first step is to do parameter initialization. Here, the weights and bias parameters for each layer are initialized as the tensor variables. \n",
        "\n",
        "Tensors are the base data structures of PyTorch which are used for building different types of neural networks. They can be considered as the generalization of arrays and matrices; in other words, tensors are N-dimensional matrices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vn0tPG0DUSC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "826ff2b2-ef81-4d4f-f18c-11b094b1362f"
      },
      "source": [
        "# initialising tensor for input and output\n",
        "X = torch.randn((1 , n_input))\n",
        "y = torch.randn((1, n_output))\n",
        "X,y"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-1.4169, -0.1208,  2.9017, -0.5320,  1.1673]]), tensor([[0.9833]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmHlvJuzECB7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "bfbb6b2b-e2ff-4804-da70-f39ec7c1a265"
      },
      "source": [
        "# initialising tensors variables for weights\n",
        "w1 = torch.randn(n_input , n_hidden)\n",
        "w2 = torch.randn(n_hidden , n_output)\n",
        "print(w1 ,\"\\n\", w2)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.1227, -1.7698, -0.4778],\n",
            "        [-0.1151, -0.4091, -1.4808],\n",
            "        [ 0.3592, -1.2880, -0.1151],\n",
            "        [-1.5897, -0.7054,  1.2197],\n",
            "        [ 1.2678,  0.9360, -1.2345]]) \n",
            " tensor([[-0.7256],\n",
            "        [ 0.2154],\n",
            "        [ 0.0544]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6wQqPImEUzw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "61f7978f-c00d-4a73-f9e6-3d9541e06c17"
      },
      "source": [
        "# initialising tensors for bias terms\n",
        "b1 = torch.randn(1, n_hidden)\n",
        "b2 = torch.randn(1, n_output)\n",
        "b1, b2"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-0.0885,  1.1882,  0.7300]]), tensor([[2.3224]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2rKqrVIE12U",
        "colab_type": "text"
      },
      "source": [
        "After the parameter initialization step, a neural network can be defined and trained in four key steps:\n",
        "\n",
        "- Forward Propagation\n",
        "- Loss computation\n",
        "- Backpropagation\n",
        "- Updating the parameters\n",
        "\n",
        "Let’s see each of these steps in a bit more detail.\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fu4PmPlOE9q1",
        "colab_type": "text"
      },
      "source": [
        "**Forward Propagation**: In this step, activations are calculated at every layer using the two steps shown below. These activations flow in the forward direction from the input layer to the output layer in order to generate the final output.\n",
        "\n",
        "1. z = weight * input + bias\n",
        "2. a = activation_function (z)\n",
        "\n",
        "The following code blocks show how we can write these steps in PyTorch. Notice that most of the functions, such as exponential and matrix multiplication, are similar to the ones in NumPy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FlIP0hMEtFv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sigmoid activation using pytorch\n",
        "def sigmoid_activation(z):\n",
        "  return 1 / (1 + torch.exp(-z))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCUYyyaIFfhL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "22da3580-7509-413d-8bbb-71a84d283afe"
      },
      "source": [
        "# activating hidden layer\n",
        "z1 = torch.mm(X, w1) + b1\n",
        "a1 = sigmoid_activation(z1)\n",
        "a1"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9697, 0.8139, 0.3020]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KivmXBwkFvgo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ae7df837-8cc7-46f0-decf-bc085ef719ce"
      },
      "source": [
        "# activation of output layer\n",
        "z2 = torch.mm(a1, w2) + b2\n",
        "output = sigmoid_activation(z2)\n",
        "output"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.8594]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxHVn5y3HodX",
        "colab_type": "text"
      },
      "source": [
        "**Loss Computation**: In this step, the error (also called loss) is calculated in the output layer. A simple loss function can tell the difference between the actual value and the predicted value. Later, we will look at different loss functions available in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9OVFXUdHupK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = y - output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2ZLywzSHoSP",
        "colab_type": "text"
      },
      "source": [
        "**Backpropagation**: The aim of this step is to minimize the error in the output layer by making marginal changes in the bias and the weights. These marginal changes are computed using the derivatives of the error term.\n",
        "\n",
        "Based on the Calculus principle of the Chain rule, the delta changes are back passed to hidden layers where corresponding changes in their weights and bias are made. This leads to an adjustment in the weights and bias until the error is minimized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-gkgewLGAPl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to calculate the derivative of activation\n",
        "def sigmoid_delta(x):\n",
        "  return x*(1-x)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "367Ph-bGMbR4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's compute the derivative of error terms\n",
        "delta_output = sigmoid_delta(output)\n",
        "delta_hidden = sigmoid_delta(a1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ez0sz-RAMuPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# backpass the changes to previous layers\n",
        "\n",
        "d_outp = loss*delta_output\n",
        "loss_h = torch.mm(d_outp , w2.t())\n",
        "d_hidn = loss_h*delta_hidden\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "336f0DZfNgpj",
        "colab_type": "text"
      },
      "source": [
        "**Updating the Parameters**: Finally, the weights and bias are updated using the delta changes received from the above backpropagation step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-yUW1wxNL7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhxTVOXENmW0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w2 += torch.mm(a1.t() , d_outp)*learning_rate \n",
        "w1 += torch.mm(X.t() , d_hidn)*learning_rate "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pmt32zC1OEcq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bebc647e-5353-417e-81d2-d08c8a6a528e"
      },
      "source": [
        "b2 += d_outp.sum() * learning_rate\n",
        "b1 += d_hidn.sum() * learning_rate\n",
        "b1, b2"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-0.0885,  1.1883,  0.7301]]), tensor([[2.3239]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjwAZCtzOeGO",
        "colab_type": "text"
      },
      "source": [
        "Finally, when these steps are executed for a number of epochs with a large number of training examples, the loss is reduced to a minimum value. The final weight and bias values are obtained which can then be used to make predictions on the unseen data.\n",
        "\n",
        "In the next cell write code to perform the above 4 steps 500 times and print the final loss, along with final weigths and bias values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36DFTUDvOZ8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-TI-Wn4QJDg",
        "colab_type": "text"
      },
      "source": [
        "T.B.C."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRgfrNS3QKL4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}